# MNIST-nn
As an introduction to deep learning, I thought it would be a good idea to build a fundamental understanding of how basic deep learning algorithms work under the hood. So, instead of using well-established Python deep learning libraries, I took it upon myself to implement gradient descent and back propagation algorithms on the MNIST digit dataset from scratch using vanilla MATLAB (https://yann.lecun.com/exdb/mnist/). After hours of pulling my hair out while debugging, I finally got it to work with 90% accuracy. The entire experience fired off countless lightbulb moments as I finally understand what it takes to "train" a computer. I hope this code can serve as a good education tool for not only those who are interested in deep learning but also those who just want to get a high level overview--I will included a lot of comments :).
